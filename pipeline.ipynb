{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from omegaconf import OmegaConf\n",
    "from tqdm import tqdm\n",
    "from einops import rearrange, repeat\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from lvdm.models.samplers.ddim import DDIMSampler\n",
    "from lvdm.models.samplers.ddim_multiplecond import DDIMSampler as DDIMSampler_multicond\n",
    "from utils.utils import instantiate_from_config\n",
    "\n",
    "from typing import List, NamedTuple\n",
    "from tqdm import tqdm\n",
    "from audiocraft.models import MusicGen\n",
    "from audiocraft.data.audio_utils import normalize_audio\n",
    "\n",
    "class Args(NamedTuple):\n",
    "    savedir:str='results/dynamicrafter_256_seed123'\n",
    "    ckpt_path:str='checkpoints/dynamicrafter_256_v1/model.ckpt'\n",
    "    config:str='configs/inference_256_v1.0.yaml'\n",
    "    prompt_dir:str='prompts/test'\n",
    "    n_samples:int=1\n",
    "    ddim_steps:int=50\n",
    "    ddim_eta:float=1.0\n",
    "    bs:int=1\n",
    "    height:int=256\n",
    "    width:int=256\n",
    "    frame_stride:int=3\n",
    "    unconditional_guidance_scale:float=7.5\n",
    "    seed:int=123\n",
    "    video_length:int=32\n",
    "    negative_prompt:bool=False\n",
    "    text_input:bool=True\n",
    "    multiple_cond_cfg:bool=False\n",
    "    cfg_img:float=None\n",
    "    timestep_spacing:str=\"uniform\"\n",
    "    guidance_rescale:float=0.0\n",
    "    perframe_ae:bool=False\n",
    "    \n",
    "    loop:bool=False\n",
    "    interp:bool=False\n",
    "\n",
    "transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))])\n",
    "\n",
    "def load_img(img_path):\n",
    "    image = Image.open(img_path).convert('RGB')\n",
    "    image_tensor = transform(image).unsqueeze(1)\n",
    "    frame_tensor = repeat(image_tensor, 'c t h w -> c (repeat t) h w', repeat=32)\n",
    "    return frame_tensor\n",
    "\n",
    "def load_model_checkpoint(model, ckpt):\n",
    "    state_dict = torch.load(ckpt, map_location=\"cpu\")\n",
    "    if \"state_dict\" in list(state_dict.keys()):\n",
    "        state_dict = state_dict[\"state_dict\"]\n",
    "        try:\n",
    "            model.load_state_dict(state_dict, strict=True)\n",
    "        except:\n",
    "            ## rename the keys for 256x256 model\n",
    "            new_pl_sd = OrderedDict()\n",
    "            for k,v in state_dict.items():\n",
    "                new_pl_sd[k] = v\n",
    "\n",
    "            for k in list(new_pl_sd.keys()):\n",
    "                if \"framestride_embed\" in k:\n",
    "                    new_key = k.replace(\"framestride_embed\", \"fps_embedding\")\n",
    "                    new_pl_sd[new_key] = new_pl_sd[k]\n",
    "                    del new_pl_sd[k]\n",
    "            model.load_state_dict(new_pl_sd, strict=True)\n",
    "    else:\n",
    "        # deepspeed\n",
    "        new_pl_sd = OrderedDict()\n",
    "        for key in state_dict['module'].keys():\n",
    "            new_pl_sd[key[16:]]=state_dict['module'][key]\n",
    "        model.load_state_dict(new_pl_sd)\n",
    "    print('>>> model checkpoint loaded.')\n",
    "    return model\n",
    "\n",
    "def get_latent_z(model, videos):\n",
    "    b, c, t, h, w = videos.shape\n",
    "    x = rearrange(videos, 'b c t h w -> (b t) c h w')\n",
    "    z = model.encode_first_stage(x)\n",
    "    z = rearrange(z, '(b t) c h w -> b c t h w', b=b, t=t)\n",
    "    return z\n",
    "\n",
    "def image_guided_synthesis(model, prompts, videos, noise_shape, n_samples=1, ddim_steps=50, ddim_eta=1., \\\n",
    "                        unconditional_guidance_scale=1.0, cfg_img=None, fs=None, text_input=False, multiple_cond_cfg=False, loop=False, interp=False, timestep_spacing='uniform', guidance_rescale=0.0, **kwargs):\n",
    "    ddim_sampler = DDIMSampler(model) if not multiple_cond_cfg else DDIMSampler_multicond(model)\n",
    "    batch_size = noise_shape[0]\n",
    "    fs = torch.tensor([fs] * batch_size, dtype=torch.long, device=model.device)\n",
    "\n",
    "    if not text_input:\n",
    "        prompts = [\"\"]*batch_size\n",
    "\n",
    "    img = videos[:,:,0] #bchw\n",
    "    img_emb = model.embedder(img) ## blc\n",
    "    img_emb = model.image_proj_model(img_emb)\n",
    "\n",
    "    cond_emb = model.get_learned_conditioning(prompts)\n",
    "    cond = {\"c_crossattn\": [torch.cat([cond_emb,img_emb], dim=1)]}\n",
    "    if model.model.conditioning_key == 'hybrid':\n",
    "        z = get_latent_z(model, videos) # b c t h w\n",
    "        if loop or interp:\n",
    "            img_cat_cond = torch.zeros_like(z)\n",
    "            img_cat_cond[:,:,0,:,:] = z[:,:,0,:,:]\n",
    "            img_cat_cond[:,:,-1,:,:] = z[:,:,-1,:,:]\n",
    "        else:\n",
    "            img_cat_cond = z[:,:,:1,:,:]\n",
    "            img_cat_cond = repeat(img_cat_cond, 'b c t h w -> b c (repeat t) h w', repeat=z.shape[2])\n",
    "        cond[\"c_concat\"] = [img_cat_cond] # b c 1 h w\n",
    "    \n",
    "    if unconditional_guidance_scale != 1.0:\n",
    "        if model.uncond_type == \"empty_seq\":\n",
    "            prompts = batch_size * [\"\"]\n",
    "            uc_emb = model.get_learned_conditioning(prompts)\n",
    "        elif model.uncond_type == \"zero_embed\":\n",
    "            uc_emb = torch.zeros_like(cond_emb)\n",
    "        uc_img_emb = model.embedder(torch.zeros_like(img)) ## b l c\n",
    "        uc_img_emb = model.image_proj_model(uc_img_emb)\n",
    "        uc = {\"c_crossattn\": [torch.cat([uc_emb,uc_img_emb],dim=1)]}\n",
    "        if model.model.conditioning_key == 'hybrid':\n",
    "            uc[\"c_concat\"] = [img_cat_cond]\n",
    "    else:\n",
    "        uc = None\n",
    "\n",
    "    ## we need one more unconditioning image=yes, text=\"\"\n",
    "    if multiple_cond_cfg and cfg_img != 1.0:\n",
    "        uc_2 = {\"c_crossattn\": [torch.cat([uc_emb,img_emb],dim=1)]}\n",
    "        if model.model.conditioning_key == 'hybrid':\n",
    "            uc_2[\"c_concat\"] = [img_cat_cond]\n",
    "        kwargs.update({\"unconditional_conditioning_img_nonetext\": uc_2})\n",
    "    else:\n",
    "        kwargs.update({\"unconditional_conditioning_img_nonetext\": None})\n",
    "\n",
    "    z0 = None\n",
    "    cond_mask = None\n",
    "\n",
    "    batch_variants = []\n",
    "    for _ in range(n_samples):\n",
    "\n",
    "        if z0 is not None:\n",
    "            cond_z0 = z0.clone()\n",
    "            kwargs.update({\"clean_cond\": True})\n",
    "        else:\n",
    "            cond_z0 = None\n",
    "        if ddim_sampler is not None:\n",
    "\n",
    "            samples, _ = ddim_sampler.sample(S=ddim_steps,\n",
    "                                            conditioning=cond,\n",
    "                                            batch_size=batch_size,\n",
    "                                            shape=noise_shape[1:],\n",
    "                                            verbose=False,\n",
    "                                            unconditional_guidance_scale=unconditional_guidance_scale,\n",
    "                                            unconditional_conditioning=uc,\n",
    "                                            eta=ddim_eta,\n",
    "                                            cfg_img=cfg_img, \n",
    "                                            mask=cond_mask,\n",
    "                                            x0=cond_z0,\n",
    "                                            fs=fs,\n",
    "                                            timestep_spacing=timestep_spacing,\n",
    "                                            guidance_rescale=guidance_rescale,\n",
    "                                            **kwargs\n",
    "                                            )\n",
    "\n",
    "        ## reconstruct from latent to pixel space\n",
    "        batch_images = model.decode_first_stage(samples)\n",
    "        batch_variants.append(batch_images)\n",
    "    ## variants, batch, c, t, h, w\n",
    "    batch_variants = torch.stack(batch_variants)\n",
    "    return batch_variants.permute(1, 0, 2, 3, 4, 5)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def inference_by_imgs(\n",
    "    args: Args,\n",
    "    imgs:List[str],\n",
    "    prompts:List[str],\n",
    "    audio_prompt:str='energetic EDM'\n",
    "):\n",
    "    # load images\n",
    "    videos = [load_img(img) for img in imgs]\n",
    "    \n",
    "    ## model config\n",
    "    config = OmegaConf.load(args.config)\n",
    "    model_config = config.pop(\"model\", OmegaConf.create())\n",
    "\n",
    "    ## set use_checkpoint as False as when using deepspeed, it encounters an error \"deepspeed backend not set\"\n",
    "    model_config['params']['unet_config']['params']['use_checkpoint'] = False\n",
    "    model = instantiate_from_config(model_config)\n",
    "    model = model.to(torch.float16).cuda()\n",
    "    model.perframe_ae = args.perframe_ae\n",
    "    assert os.path.exists(args.ckpt_path), \"Error: checkpoint Not Found!\"\n",
    "    model = load_model_checkpoint(model, args.ckpt_path)\n",
    "    model.eval()\n",
    "    \n",
    "    ## latent noise shape\n",
    "    h, w = args.height // 8, args.width // 8\n",
    "    channels = model.model.diffusion_model.out_channels\n",
    "    n_frames = args.video_length\n",
    "    print(f'Inference with {n_frames} frames')\n",
    "    noise_shape = [args.bs, channels, n_frames, h, w]\n",
    "    \n",
    "    frames = []\n",
    "    with torch.cuda.amp.autocast():\n",
    "        for prompt, video in tqdm(zip(prompts, videos)):\n",
    "            video = video.unsqueeze(0).to(torch.float16).cuda()\n",
    "            prompt = [prompt]\n",
    "            batch_sampler = image_guided_synthesis(model, prompt, video, noise_shape, args.n_samples, args.ddim_steps, args.ddim_eta, \\\n",
    "                                args.unconditional_guidance_scale, args.cfg_img, args.frame_stride, args.text_input, args.multiple_cond_cfg, args.loop, args.interp, args.timestep_spacing, args.guidance_rescale)\n",
    "            frames.append(batch_sampler.squeeze().cpu())\n",
    "\n",
    "    # list[tensor] -> tensor    \n",
    "    video = torch.concat(frames, dim=1)\n",
    "    video = torch.clamp(video.float(), -1., 1.)\n",
    "    video = (video + 1.0) / 2.0\n",
    "    video = (video * 255).to(torch.uint8).permute(1,2,3,0) #thwc\n",
    "    \n",
    "    video_time = len(video) / 8\n",
    "    \n",
    "    aud_model = MusicGen.get_pretrained('facebook/musicgen-melody')\n",
    "    aud_model.set_generation_params(duration=video_time)\n",
    "    wav = aud_model.generate([audio_prompt])\n",
    "    wav = wav.squeeze(0).cpu()\n",
    "    norm_wav = normalize_audio(\n",
    "        wav=wav,\n",
    "        normalize=True,\n",
    "        strategy='loudness',\n",
    "        peak_clip_headroom_db=1,\n",
    "        rms_headroom_db=18,\n",
    "        loudness_headroom_db=14,\n",
    "        loudness_compressor=True,\n",
    "        log_clipping=True,\n",
    "        sample_rate=aud_model.sample_rate,\n",
    "        stem_name='testout')\n",
    "    \n",
    "    torchvision.io.write_video('testout_aud.mp4', video, fps=8, video_codec='h264', options={'crf': '10'},\n",
    "                           audio_array=norm_wav, audio_fps=aud_model.sample_rate, audio_codec='aac')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "videopipeline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
